{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:34:50.426781Z",
     "start_time": "2024-06-06T04:34:45.391366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments"
   ],
   "id": "85f7f20d99d50f87",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:34:50.466085Z",
     "start_time": "2024-06-06T04:34:50.427690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set the file path\n",
    "train_path = '/Users/hoshea/Documents/pythonProject/NLP/train.jsonl'\n",
    "test_path =  '/Users/hoshea/Documents/pythonProject/NLP/eval.jsonl'\n",
    "\n",
    "# read the file into a pandas DataFrame\n",
    "# df_train = pd.read_json(train_path,lines = True)\n",
    "# df_test = pd.read_json(test_path,lines = True)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:34:50.468543Z",
     "start_time": "2024-06-06T04:34:50.466908Z"
    }
   },
   "cell_type": "code",
   "source": "#pip install 'transformers[torch]' accelerate -U",
   "id": "b005e435d6ecfb86",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:34:50.470928Z",
     "start_time": "2024-06-06T04:34:50.469075Z"
    }
   },
   "cell_type": "code",
   "source": "#pip install sentencepiece",
   "id": "c18d28eb25c87511",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T04:35:16.614533Z",
     "start_time": "2024-06-06T04:34:50.472115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "data_files = {\"train\": train_path}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "# split the data\n",
    "def train_test_split(dataset, test_size=0.2):\n",
    "    train_test = dataset['train'].train_test_split(test_size=test_size)\n",
    "    return DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'test': train_test['test']\n",
    "    })\n",
    "dataset = train_test_split(dataset)\n",
    "\n",
    "# import tokenizer\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = examples[\"Question\"]\n",
    "    choices = [[examples[\"Alternative1\"][i], examples[\"Alternative2\"][i]] for i in range(len(questions))]\n",
    "    labels = [label - 1 for label in examples[\"Answer\"]]  # Â∞Ü1/2ËΩ¨Êç¢‰∏∫0/1\n",
    "\n",
    "    for label in labels:\n",
    "        if label not in [0, 1]:\n",
    "            raise ValueError(f\"Unexpected label value: {label}\")\n",
    "\n",
    "    contexts = []\n",
    "    for question, choice_pair in zip(questions, choices):\n",
    "        for choice in choice_pair:\n",
    "            contexts.append((question, choice))\n",
    "\n",
    "    tokenized_examples = tokenizer(*zip(*contexts), truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    input_ids = [tokenized_examples['input_ids'][i:i+2] for i in range(0, len(tokenized_examples['input_ids']), 2)]\n",
    "    attention_mask = [tokenized_examples['attention_mask'][i:i+2] for i in range(0, len(tokenized_examples['attention_mask']), 2)]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids),\n",
    "        'attention_mask': torch.tensor(attention_mask),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "# preprocess\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)"
   ],
   "id": "dcebf89957559f27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efc56925bfbf4639bc714c223c9d57f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoshea/Documents/pythonProject/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fae26da8a1a64783b34dd1e1e33c80df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "786192c617cb45b4a0560dd494d29856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoshea/Documents/pythonProject/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/11929 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26db0d26c4c943bf92c23cda77bace08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2983 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2db9d245441b4ee8a8f3918230aa7105"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "636b891e04484f7e801d77cbbc894349"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T07:02:54.790488Z",
     "start_time": "2024-06-06T04:35:16.615317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # Âè™‰øùÁïôÊúÄÊñ∞ÁöÑÊ£ÄÊü•ÁÇπ\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,  # Áî±‰∫éÂ§öÈÄâ‰ªªÂä°ÂÜÖÂ≠òÊ∂àËÄóËæÉÂ§ßÔºåÈÄÇÂΩìÂáèÂ∞èbatch size\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# save the best model\n",
    "trainer.save_model(\"./best_model\")"
   ],
   "id": "ec5448f44b3f8ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoshea/Documents/pythonProject/venv/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17895' max='17895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17895/17895 2:27:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.721564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>1.084918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>1.421945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T07:08:37.100146Z",
     "start_time": "2024-06-06T07:05:25.365185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def load_eval_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    return data\n",
    "\n",
    "eval_data = load_eval_data(test_path)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "# import tokenizer and model\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"./best_model\")  # ‰ΩøÁî®‰øùÂ≠òÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ\n",
    "\n",
    "# Preprocess for evaluation\n",
    "def preprocess_function(examples):\n",
    "    questions = examples[\"Question\"]\n",
    "    choices = [[examples[\"Alternative1\"][i], examples[\"Alternative2\"][i]] for i in range(len(questions))]\n",
    "\n",
    "    contexts = []\n",
    "    for question, choice_pair in zip(questions, choices):\n",
    "        for choice in choice_pair:\n",
    "            contexts.append((question, choice))\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(*zip(*contexts), truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    input_ids = [tokenized_examples['input_ids'][i:i+2] for i in range(0, len(tokenized_examples['input_ids']), 2)]\n",
    "    attention_mask = [tokenized_examples['attention_mask'][i:i+2] for i in range(0, len(tokenized_examples['attention_mask']), 2)]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Build dataset for evaluation\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_df.columns.tolist())\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "\n",
    "predicted_logits = np.array(predictions.predictions)\n",
    "predicted_labels = np.argmax(predicted_logits, axis=1).tolist()\n",
    "\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    'ID': eval_df['Id'],\n",
    "    'Target': [label + 1 for label in predicted_labels]  # Â∞ÜÊ†áÁ≠æ‰ªé0/1ËΩ¨Êç¢Âõû1/2\n",
    "})\n",
    "\n",
    "# save the result\n",
    "output.to_csv('eval_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to eval_predictions.csv\")\n"
   ],
   "id": "d9b279c596b77beb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoshea/Documents/pythonProject/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4261 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3df6f0a713ca48d9a94f4f81db64f01b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to eval_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f5f7300a5ab4c362"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
